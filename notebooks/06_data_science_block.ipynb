{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPxC1RWWI1XcB4zKbXylmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rvkushnir/project_fifa_players/blob/main/notebooks/06_data_science_block.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ініціалізація середовища**\n",
        "\n",
        "Клонуємо репозиторій з GitHub у /content, переходимо в корінь проєкту.\n",
        "Це гарантує однакові шляхи для даних/виводів у всіх сесіях Colab.\n",
        "Після клонування перевіряємо наявність data/raw/fifa_players.csv.\n",
        "Вихід: робочий каталог → pwd = /content/project_fifa_players."
      ],
      "metadata": {
        "id": "iiecaHqM1XP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Ініціалізація середовища: клон репозиторію і перехід у робочу теку\n",
        "REPO = \"rvkushnir/project_fifa_players\"\n",
        "BRANCH = \"main\"\n",
        "WORKDIR = \"/content/project_fifa_players\"\n",
        "\n",
        "# чистий клон\n",
        "!rm -rf \"$WORKDIR\"\n",
        "!git clone --depth 1 -b \"$BRANCH\" \"https://github.com/{REPO}.git\" \"$WORKDIR\"\n",
        "%cd \"$WORKDIR\"\n",
        "\n",
        "# Якщо у репо використовується Git LFS (великі файли) — підтягнемо дані\n",
        "!git lfs install 2>/dev/null || true\n",
        "!git lfs pull 2>/dev/null || true\n",
        "\n",
        "# Перевіримо, що CSV на місці\n",
        "!ls -lah data/raw\n"
      ],
      "metadata": {
        "id": "YBT_OIFl1W80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета кроку. Підключення бібліотек, налаштування шляхів та базові константи/множини позицій. Ця клітинка нічого не читає і не пише — лише готує середовище."
      ],
      "metadata": {
        "id": "KS5IdO0aJP0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtjFtVtx1Ku3"
      },
      "outputs": [],
      "source": [
        "# === 17–19: EDA → Feature Engineering → Feature Selection\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, re, json\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "DATA_CSV = Path(\"data/raw/fifa_players.csv\")\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#  Допоміжні\n",
        "DEF = {\"CB\",\"LB\",\"RB\",\"LWB\",\"RWB\"}\n",
        "MID = {\"CDM\",\"CM\",\"CAM\",\"LM\",\"RM\"}\n",
        "FWD = {\"ST\",\"CF\",\"LW\",\"RW\"}\n",
        "\n",
        "# Канонічні 15 позицій\n",
        "CANON15 = [\"GK\",\"CB\",\"LB\",\"RB\",\"LWB\",\"RWB\",\"CDM\",\"CM\",\"CAM\",\"LM\",\"RM\",\"LW\",\"RW\",\"ST\",\"CF\"]\n",
        "\n",
        "# Патч для OHE (щоб працювало і в нових, і в старих sklearn)\n",
        "def OHE_dense():\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def make_primary_position(s: pd.Series) -> pd.Series:\n",
        "    # перша позиція як primary\n",
        "    return s.astype(str).str.split(\",\", n=1, expand=True)[0].str.strip()\n",
        "\n",
        "def role_group(pos: str) -> str:\n",
        "    p = str(pos).upper()\n",
        "    if p==\"GK\": return \"GK\"\n",
        "    if p in DEF: return \"DEF\"\n",
        "    if p in MID: return \"MID\"\n",
        "    if p in FWD: return \"FWD\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "def safe_num(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def parse_work_rate(s):\n",
        "    # \"High/Med\" -> (2,1), H=2,M=1,L=0\n",
        "    m = {\"High\":2,\"Med\":1,\"Low\":0}\n",
        "    if pd.isna(s): return (np.nan, np.nan)\n",
        "    parts = str(s).split(\"/\")\n",
        "    a = m.get(parts[0].strip().title(), np.nan) if len(parts)>0 else np.nan\n",
        "    d = m.get(parts[1].strip().title(), np.nan) if len(parts)>1 else np.nan\n",
        "    return (a,d)\n",
        "\n",
        "def pct(x): return (x.isna().mean() if hasattr(x,\"isna\") else np.nan)\n",
        "\n",
        "def corr_pairs(df, cols, thr=0.9):\n",
        "    c = df[cols].corr(numeric_only=True)\n",
        "    pairs = []\n",
        "    for i,a in enumerate(cols):\n",
        "        for j,b in enumerate(cols):\n",
        "            if j<=i: continue\n",
        "            r = c.loc[a,b]\n",
        "            if pd.notna(r) and abs(r)>=thr:\n",
        "                pairs.append((a,b,round(float(r),3)))\n",
        "    return sorted(pairs, key=lambda t: -abs(t[2]))\n",
        "\n",
        "#  Завантаження\n",
        "if not DATA_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Не знайдено файл: {DATA_CSV.resolve()}\")\n",
        "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
        "\n",
        "# Базові колонки\n",
        "must = [\"sofifa_id\",\"player_url\",\"short_name\",\"long_name\",\"player_positions\",\n",
        "        \"overall\",\"age\",\"height_cm\",\"weight_kg\",\"value_eur\",\"wage_eur\",\n",
        "        \"club_name\",\"league_name\",\"preferred_foot\",\"weak_foot\",\"work_rate\"]\n",
        "for m in must:\n",
        "    if m not in df.columns: print(f\"[WARN] Відсутня колонка: {m}\")\n",
        "\n",
        "# Техпідготовка\n",
        "df[\"primary_position\"] = make_primary_position(df[\"player_positions\"])\n",
        "df[\"role_group\"] = df[\"primary_position\"].apply(role_group)\n",
        "safe_num(df, [\"overall\",\"age\",\"height_cm\",\"weight_kg\",\"value_eur\",\"wage_eur\",\"weak_foot\"])\n",
        "\n",
        "#  17) EDA\n",
        "# Типи/пропуски\n",
        "nulls = df.isna().mean().sort_values(ascending=False).to_frame(\"null_frac\")\n",
        "types = df.dtypes.astype(str).to_frame(\"dtype\")\n",
        "types.to_csv(OUT_T/\"EDA_types.csv\")\n",
        "nulls.to_csv(OUT_T/\"EDA_nulls.csv\")\n",
        "\n",
        "# Дублікати\n",
        "dup_id = df.duplicated(subset=[\"sofifa_id\"]).sum() if \"sofifa_id\" in df.columns else np.nan\n",
        "dup_url = df.duplicated(subset=[\"player_url\"]).sum() if \"player_url\" in df.columns else np.nan\n",
        "dup_name = df.duplicated(subset=[\"short_name\",\"dob\"]).sum() if \"dob\" in df.columns else df.duplicated(subset=[\"short_name\"]).sum()\n",
        "pd.DataFrame([{\"dup_sofifa_id\":int(dup_id), \"dup_url\":int(dup_url), \"dup_name\":int(dup_name)}]).to_csv(OUT_T/\"EDA_duplicates.csv\", index=False)\n",
        "\n",
        "# Діапазони/квантили\n",
        "num_core = [c for c in [\"age\",\"overall\",\"height_cm\",\"weight_kg\",\"value_eur\",\"wage_eur\"] if c in df.columns]\n",
        "qtab = df[num_core].quantile([0.01,0.05,0.5,0.95,0.99]).T\n",
        "qtab.to_csv(OUT_T/\"EDA_quantiles_core.csv\")\n",
        "\n",
        "# Валідність\n",
        "viol = {\n",
        "    \"age_out\": int((~df[\"age\"].between(15,45)).sum()) if \"age\" in df.columns else np.nan,\n",
        "    \"height_out\": int((~df[\"height_cm\"].between(150,210)).sum()) if \"height_cm\" in df.columns else np.nan,\n",
        "    \"weight_out\": int((~df[\"weight_kg\"].between(45,120)).sum()) if \"weight_kg\" in df.columns else np.nan,\n",
        "    \"value_neg_or_zero\": int((df[\"value_eur\"]<=0).sum()) if \"value_eur\" in df.columns else np.nan,\n",
        "    \"wage_neg_or_zero\": int((df[\"wage_eur\"]<=0).sum()) if \"wage_eur\" in df.columns else np.nan,\n",
        "}\n",
        "pd.DataFrame([viol]).to_csv(OUT_T/\"EDA_range_violations.csv\", index=False)\n",
        "\n",
        "# Розподіл позицій/груп\n",
        "pos_count = df[\"primary_position\"].value_counts(dropna=False).to_frame(\"n\")\n",
        "grp_count = df[\"role_group\"].value_counts(dropna=False).to_frame(\"n\")\n",
        "pos_count.to_csv(OUT_T/\"EDA_pos_count.csv\")\n",
        "grp_count.to_csv(OUT_T/\"EDA_role_group_count.csv\")\n",
        "\n",
        "# Контамінація ознак GK vs польові\n",
        "GK_COLS = [c for c in df.columns if c.startswith(\"goalkeeping_\")]\n",
        "FIELD_GROUP_PREFIXES = [\"attacking_\",\"skill_\",\"movement_\",\"power_\",\"mentality_\",\"defending_\"]\n",
        "FIELD_DET = [c for c in df.columns if any(c.startswith(p) for p in FIELD_GROUP_PREFIXES)]\n",
        "contam = {}\n",
        "if GK_COLS:\n",
        "    non_gk = df[df[\"role_group\"]!=\"GK\"]\n",
        "    gk = df[df[\"role_group\"]==\"GK\"]\n",
        "    contam[\"nonGK_has_GK_notnull_frac\"] = float(non_gk[GK_COLS].notna().mean().mean())\n",
        "    contam[\"GK_has_FIELD_notnull_frac\"] = float(gk[FIELD_DET].notna().mean().mean()) if FIELD_DET else np.nan\n",
        "pd.DataFrame([contam]).to_csv(OUT_T/\"EDA_contamination.csv\", index=False)\n",
        "\n",
        "# Кореляції ядра й сильні пари\n",
        "core = [c for c in [\"pace\",\"shooting\",\"passing\",\"dribbling\",\"defending\",\"physic\",\"overall\"] if c in df.columns]\n",
        "strong_pairs = corr_pairs(df, core, thr=0.9) if len(core)>=2 else []\n",
        "pd.DataFrame(strong_pairs, columns=[\"f1\",\"f2\",\"r\"]).to_csv(OUT_T/\"EDA_core_strong_corr.csv\", index=False)\n",
        "\n",
        "# Кардинальності категорій\n",
        "cat_cols = [c for c in [\"club_name\",\"league_name\",\"nationality_name\",\"preferred_foot\",\"work_rate\",\"body_type\"] if c in df.columns]\n",
        "card = {c:int(df[c].nunique(dropna=True)) for c in cat_cols}\n",
        "pd.DataFrame([card]).to_csv(OUT_T/\"EDA_categorical_cardinality.csv\", index=False)\n",
        "\n",
        "# Потенційні leakage-фічі (списком)\n",
        "leak_like = [c for c in df.columns if c in [\"club_position\",\"nation_position\",\"club_jersey_number\",\"nation_jersey_number\"]]\n",
        "pd.Series(leak_like, name=\"leak_like\").to_csv(OUT_T/\"EDA_potential_leakage.csv\", index=False)\n",
        "\n",
        "print(\"[EDA] OK. Збережено таблиці в out/tables/ (EDA_*)\")\n",
        "\n",
        "#  18) Feature Engineering\n",
        "# Цільові мапи\n",
        "def map_to_15(p):\n",
        "    p = str(p).upper()\n",
        "    return p if p in CANON15 else p  # лишаємо як є; згодом можна нормалізувати \"RB/LB/...\" суворо\n",
        "\n",
        "df[\"target_pos15\"] = df[\"primary_position\"].str.upper()\n",
        "df[\"target_grp4\"]  = df[\"role_group\"]\n",
        "\n",
        "# Індекси-агрегати (беремо що є)\n",
        "def mean_cols(row, cols):\n",
        "    vals = [row[c] for c in cols if c in row.index and pd.notna(row[c])]\n",
        "    return np.mean(vals) if vals else np.nan\n",
        "\n",
        "# Переліки груп\n",
        "PACE = [c for c in [\"movement_acceleration\",\"movement_sprint_speed\",\"pace\"] if c in df.columns]\n",
        "DRIB = [c for c in [\"dribbling\",\"skill_dribbling\",\"skill_ball_control\",\"movement_agility\",\"movement_balance\"] if c in df.columns]\n",
        "PASS = [c for c in [\"passing\",\"attacking_short_passing\",\"skill_long_passing\",\"mentality_vision\",\"skill_curve\",\"attacking_crossing\"] if c in df.columns]\n",
        "SHOOT = [c for c in [\"shooting\",\"attacking_finishing\",\"attacking_volleys\",\"power_shot_power\",\"power_long_shots\",\"attacking_heading_accuracy\",\"mentality_penalties\"] if c in df.columns]\n",
        "DEFND = [c for c in [\"defending\",\"defending_marking_awareness\",\"defending_standing_tackle\",\"defending_sliding_tackle\",\"mentality_interceptions\",\"power_strength\"] if c in df.columns]\n",
        "PHYS = [c for c in [\"physic\",\"power_stamina\",\"power_strength\",\"power_jumping\"] if c in df.columns]\n",
        "GKSET = [c for c in [\"goalkeeping_diving\",\"goalkeeping_reflexes\",\"goalkeeping_positioning\",\"goalkeeping_handling\",\"goalkeeping_kicking\",\"goalkeeping_speed\"] if c in df.columns]\n",
        "\n",
        "for grp, cols in [(\"pace_idx\", PACE),(\"dribble_idx\", DRIB),(\"playmake_idx\", PASS),\n",
        "                  (\"attack_idx\", SHOOT),(\"defend_idx\", DEFND),(\"phys_idx\", PHYS),(\"gk_idx\", GKSET)]:\n",
        "    if cols:\n",
        "        df[grp] = df[cols].mean(axis=1, skipna=True)\n",
        "\n",
        "# BMI, age^2\n",
        "if \"height_cm\" in df.columns and \"weight_kg\" in df.columns:\n",
        "    h_m = df[\"height_cm\"]/100.0\n",
        "    df[\"bmi\"] = df[\"weight_kg\"]/(h_m*h_m)\n",
        "if \"age\" in df.columns:\n",
        "    df[\"age2\"] = df[\"age\"]**2\n",
        "\n",
        "# work_rate → два числових\n",
        "if \"work_rate\" in df.columns:\n",
        "    wr = df[\"work_rate\"].apply(parse_work_rate)\n",
        "    df[\"work_rate_att\"] = wr.apply(lambda t: t[0])\n",
        "    df[\"work_rate_def\"] = wr.apply(lambda t: t[1])\n",
        "\n",
        "# preferred_foot → is_left, weak_foot\n",
        "if \"preferred_foot\" in df.columns:\n",
        "    df[\"is_left\"] = (df[\"preferred_foot\"].str.upper()==\"LEFT\").astype(\"Int64\")\n",
        "\n",
        "# Легка імпутація числових: медіана за role_group → глобальна\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_cols = [c for c in num_cols if c not in [\"sofifa_id\"]]  # захист id\n",
        "med_by_role = df.groupby(\"role_group\")[num_cols].median(numeric_only=True)\n",
        "def impute_row(row):\n",
        "    rg = row[\"role_group\"]\n",
        "    for c in num_cols:\n",
        "        if pd.isna(row[c]):\n",
        "            if rg in med_by_role.index and pd.notna(med_by_role.loc[rg, c]):\n",
        "                row[c] = med_by_role.loc[rg, c]\n",
        "    return row\n",
        "df_num = df.copy()\n",
        "df_num[num_cols] = df_num[num_cols].astype(float)\n",
        "df_num = df_num.apply(impute_row, axis=1)\n",
        "for c in num_cols:\n",
        "    df_num[c] = df_num[c].fillna(df_num[c].median())  # остаточний fallback\n",
        "\n",
        "# Вибір «безпечних» фіч (без ідентифікаторів/URL/ліків)\n",
        "drop_like = set([\n",
        "    \"player_url\",\"long_name\",\"short_name\",\"club_logo_url\",\"nation_logo_url\",\"nation_flag_url\",\n",
        "    \"club_position\",\"nation_position\",\"club_jersey_number\",\"nation_jersey_number\"\n",
        "])\n",
        "keep_num = [c for c in num_cols if c not in drop_like]\n",
        "keep_cat = [c for c in [\"preferred_foot\",\"work_rate\",\"body_type\",\"league_name\",\"club_name\",\"nationality_name\"] if c in df.columns]\n",
        "\n",
        "fe_base = df_num[[\"sofifa_id\",\"primary_position\",\"target_pos15\",\"target_grp4\"] + keep_num + keep_cat]\n",
        "fe_base.to_csv(OUT_T/\"Task18_features_base.csv\", index=False)\n",
        "\n",
        "print(\"[FE] OK. Збережено базову матрицю ознак: Task18_features_base.csv\")\n",
        "\n",
        "#  19) Feature Selection\n",
        "# Мішень: мультиклас за target_pos15 (фільтруємо тільки канонічні)\n",
        "fe = fe_base[fe_base[\"target_pos15\"].isin(CANON15)].copy()\n",
        "y = fe[\"target_pos15\"].astype(str)\n",
        "\n",
        "# Підготовка X: числові + one-hot категорій\n",
        "X_num = fe[keep_num].copy()\n",
        "X_cat = fe[keep_cat].astype(str) if keep_cat else None\n",
        "\n",
        "if X_cat is not None and len(keep_cat)>0:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    X_cat_arr = ohe.fit_transform(X_cat)\n",
        "    ohe_cols = ohe.get_feature_names_out(keep_cat).tolist()\n",
        "    X_cat_df = pd.DataFrame(X_cat_arr, columns=ohe_cols, index=fe.index)\n",
        "    X_full = pd.concat([X_num, X_cat_df], axis=1)\n",
        "else:\n",
        "    X_full = X_num.copy()\n",
        "    ohe_cols = []\n",
        "\n",
        "# Видалення надсильних кореляцій (|r|≥0.95) серед числових\n",
        "strong = corr_pairs(X_num, X_num.columns.tolist(), thr=0.95)\n",
        "drop_corr = set([b for (a,b,_) in strong])\n",
        "X_filt = X_full.drop(columns=list(drop_corr), errors=\"ignore\")\n",
        "\n",
        "# MI (фільтраційно)\n",
        "mi = mutual_info_classif(X_filt.fillna(0), y, discrete_features=[c in ohe_cols for c in X_filt.columns], random_state=42)\n",
        "mi_tbl = pd.DataFrame({\"feature\": X_filt.columns, \"mi\": mi}).sort_values(\"mi\", ascending=False)\n",
        "mi_tbl.to_csv(OUT_T/\"Task19_mi_features_pos15.csv\", index=False)\n",
        "\n",
        "# RF (вбудоване)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "rf.fit(X_filt.fillna(0), y)\n",
        "rf_imp = pd.DataFrame({\"feature\": X_filt.columns, \"rf_importance\": rf.feature_importances_}) \\\n",
        "         .sort_values(\"rf_importance\", ascending=False)\n",
        "rf_imp.to_csv(OUT_T/\"Task19_rf_importance_pos15.csv\", index=False)\n",
        "\n",
        "# Рекомендований набір: перетин топ-N (MI, RF), без корельованих\n",
        "TOPN = 60\n",
        "top_mi = set(mi_tbl.head(TOPN)[\"feature\"])\n",
        "top_rf = set(rf_imp.head(TOPN)[\"feature\"])\n",
        "sel = sorted((top_mi & top_rf) - drop_corr)\n",
        "\n",
        "pd.Series(sel, name=\"selected_feature\").to_csv(OUT_T/\"Task19_selected_features_pos15.csv\", index=False)\n",
        "\n",
        "print(\"[FS] OK. MI/RF/selected збережені в out/tables/. Рекомендовано використати Task19_selected_features_pos15.csv\")\n",
        "# =============================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета. Оголосити утиліту для пошуку «сильних» кореляцій (Pearson/Spearman) між усіма числовими навичками. УВАГА: ця клітинка лише визначає функцію — ми її викличемо в наступній клітинці, де вже є df."
      ],
      "metadata": {
        "id": "2Gm48I2HJoxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Розширений пошук сильних кореляцій (усі числові навички) ---\n",
        "\n",
        "\n",
        "def save_strong_corr(df: pd.DataFrame, out_dir: Path, thr: float = 0.90):\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    EXCLUDE_LIKE = {\n",
        "        \"sofifa_id\",\"overall\",\"value_eur\",\"wage_eur\",  # лишаємо окремо від «технічних»\n",
        "        \"club_jersey_number\",\"nation_jersey_number\",\n",
        "    }\n",
        "\n",
        "    # лише «технічні» числові фічі\n",
        "    num_all = [c for c in df.select_dtypes(include=[np.number]).columns if c not in EXCLUDE_LIKE]\n",
        "    if not num_all:\n",
        "        print(\"[EDA] Немає числових колонок для кореляцій.\")\n",
        "        # все одно створимо порожні файли з заголовками\n",
        "        pd.DataFrame(columns=[\"f1\",\"f2\",\"|r|\"]).to_csv(out_dir/\"EDA_strong_corr_all_pearson.csv\", index=False)\n",
        "        pd.DataFrame(columns=[\"f1\",\"f2\",\"|rho|\"]).to_csv(out_dir/\"EDA_strong_corr_all_spearman.csv\", index=False)\n",
        "        return\n",
        "\n",
        "    # Pearson\n",
        "    corr_p = df[num_all].corr(numeric_only=True).abs()\n",
        "    pairs_p = []\n",
        "    for i, a in enumerate(num_all):\n",
        "        for j, b in enumerate(num_all):\n",
        "            if j <= i:\n",
        "                continue\n",
        "            r = corr_p.loc[a, b]\n",
        "            if pd.notna(r) and r >= thr:\n",
        "                pairs_p.append((a, b, round(float(r), 3)))\n",
        "    pearson_path = out_dir/\"EDA_strong_corr_all_pearson.csv\"\n",
        "    pd.DataFrame(pairs_p, columns=[\"f1\",\"f2\",\"|r|\"]).sort_values(\"|r|\", ascending=False).to_csv(pearson_path, index=False)\n",
        "\n",
        "    # Spearman (монотонний зв'язок)\n",
        "    corr_s = df[num_all].rank().corr(numeric_only=True).abs()\n",
        "    pairs_s = []\n",
        "    for i, a in enumerate(num_all):\n",
        "        for j, b in enumerate(num_all):\n",
        "            if j <= i:\n",
        "                continue\n",
        "            r = corr_s.loc[a, b]\n",
        "            if pd.notna(r) and r >= thr:\n",
        "                pairs_s.append((a, b, round(float(r), 3)))\n",
        "    spearman_path = out_dir/\"EDA_strong_corr_all_spearman.csv\"\n",
        "    pd.DataFrame(pairs_s, columns=[\"f1\",\"f2\",\"|rho|\"]).sort_values(\"|rho|\", ascending=False).to_csv(spearman_path, index=False)\n",
        "\n",
        "    print(f\"[EDA] strong corr saved (thr={thr}) →\")\n",
        "    print(f\"  • Pearson:  {len(pairs_p)} пар  → {pearson_path}\")\n",
        "    print(f\"  • Spearman: {len(pairs_s)} пар  → {spearman_path}\")\n"
      ],
      "metadata": {
        "id": "Ww-CpumT6v8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета.\n",
        "\n",
        "EDA (Task 17): завантажити csv, зібрати базові перевірки якості, розрахувати квантили/діапазони, частки пропусків, підрахунки позицій/груп, перевірити «контамінацію» GK/польових, базові кореляції ядра, потенційні leakage-фічі та зберегти повні матриці сильних кореляцій (викликаємо нашу функцію).\n",
        "\n",
        "Feature Engineering (Task 18): побудувати індекси (pace/attack/defend/…), bmi, age2, декодувати work_rate, легка імпутація по role_group, сформувати Task18_features_base.csv.\n",
        "\n",
        "Feature Selection (Task 19): OHE-категорій, дроп надсильних корельованих числових (|r|≥0.95), MI та RF-важливості, і рекомендований набір як перетин топ-N (MI, RF) без надлишкових фіч."
      ],
      "metadata": {
        "id": "MiFs1tU4J8L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Допоміжні для 3-ї клітинки\n",
        "def make_primary_position(s: pd.Series) -> pd.Series:\n",
        "    # перша позиція як primary\n",
        "    return s.astype(str).str.split(\",\", n=1, expand=True)[0].str.strip()\n",
        "\n",
        "def role_group(pos: str) -> str:\n",
        "    p = str(pos).upper()\n",
        "    if p==\"GK\": return \"GK\"\n",
        "    if p in DEF: return \"DEF\"\n",
        "    if p in MID: return \"MID\"\n",
        "    if p in FWD: return \"FWD\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "def safe_num(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def parse_work_rate(s):\n",
        "    # \"High/Med\" -> (2,1), H=2,M=1,L=0\n",
        "    m = {\"High\":2,\"Med\":1,\"Low\":0}\n",
        "    if pd.isna(s): return (np.nan, np.nan)\n",
        "    parts = str(s).split(\"/\")\n",
        "    a = m.get(parts[0].strip().title(), np.nan) if len(parts)>0 else np.nan\n",
        "    d = m.get(parts[1].strip().title(), np.nan) if len(parts)>1 else np.nan\n",
        "    return (a,d)\n",
        "\n",
        "def corr_pairs(df, cols, thr=0.9):\n",
        "    c = df[cols].corr(numeric_only=True)\n",
        "    pairs = []\n",
        "    for i,a in enumerate(cols):\n",
        "        for j,b in enumerate(cols):\n",
        "            if j<=i: continue\n",
        "            r = c.loc[a,b]\n",
        "            if pd.notna(r) and abs(r)>=thr:\n",
        "                pairs.append((a,b,round(float(r),3)))\n",
        "    return sorted(pairs, key=lambda t: -abs(t[2]))\n",
        "\n",
        "#  Завантаження\n",
        "if not DATA_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Не знайдено файл: {DATA_CSV.resolve()}\")\n",
        "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
        "\n",
        "# Базові колонки\n",
        "must = [\"sofifa_id\",\"player_url\",\"short_name\",\"long_name\",\"player_positions\",\n",
        "        \"overall\",\"age\",\"height_cm\",\"weight_kg\",\"value_eur\",\"wage_eur\",\n",
        "        \"club_name\",\"league_name\",\"preferred_foot\",\"weak_foot\",\"work_rate\"]\n",
        "for m in must:\n",
        "    if m not in df.columns: print(f\"[WARN] Відсутня колонка: {m}\")\n",
        "\n",
        "# Техпідготовка\n",
        "df[\"primary_position\"] = make_primary_position(df[\"player_positions\"])\n",
        "df[\"role_group\"] = df[\"primary_position\"].apply(role_group)\n",
        "safe_num(df, [\"overall\",\"age\",\"height_cm\",\"weight_kg\",\"value_eur\",\"wage_eur\",\"weak_foot\"])\n",
        "\n",
        "#  17) EDA\n",
        "# Типи/пропуски\n",
        "nulls = df.isna().mean().sort_values(ascending=False).to_frame(\"null_frac\")\n",
        "types = df.dtypes.astype(str).to_frame(\"dtype\")\n",
        "types.to_csv(OUT_T/\"EDA_types.csv\")\n",
        "nulls.to_csv(OUT_T/\"EDA_nulls.csv\")\n",
        "\n",
        "# Дублікати\n",
        "dup_id = df.duplicated(subset=[\"sofifa_id\"]).sum() if \"sofifa_id\" in df.columns else np.nan\n",
        "dup_url = df.duplicated(subset=[\"player_url\"]).sum() if \"player_url\" in df.columns else np.nan\n",
        "dup_name = df.duplicated(subset=[\"short_name\",\"dob\"]).sum() if \"dob\" in df.columns else df.duplicated(subset=[\"short_name\"]).sum()\n",
        "pd.DataFrame([{\"dup_sofifa_id\":int(dup_id), \"dup_url\":int(dup_url), \"dup_name\":int(dup_name)}]).to_csv(OUT_T/\"EDA_duplicates.csv\", index=False)\n",
        "\n",
        "# Діапазони/квантили\n",
        "num_core = [c for c in [\"age\",\"overall\",\"height_cm\",\"weight_kg\",\"value_eur\",\"wage_eur\"] if c in df.columns]\n",
        "qtab = df[num_core].quantile([0.01,0.05,0.5,0.95,0.99]).T\n",
        "qtab.to_csv(OUT_T/\"EDA_quantiles_core.csv\")\n",
        "\n",
        "# Валідність\n",
        "viol = {\n",
        "    \"age_out\": int((~df[\"age\"].between(15,45)).sum()) if \"age\" in df.columns else np.nan,\n",
        "    \"height_out\": int((~df[\"height_cm\"].between(150,210)).sum()) if \"height_cm\" in df.columns else np.nan,\n",
        "    \"weight_out\": int((~df[\"weight_kg\"].between(45,120)).sum()) if \"weight_kg\" in df.columns else np.nan,\n",
        "    \"value_neg_or_zero\": int((df[\"value_eur\"]<=0).sum()) if \"value_eur\" in df.columns else np.nan,\n",
        "    \"wage_neg_or_zero\": int((df[\"wage_eur\"]<=0).sum()) if \"wage_eur\" in df.columns else np.nan,\n",
        "}\n",
        "pd.DataFrame([viol]).to_csv(OUT_T/\"EDA_range_violations.csv\", index=False)\n",
        "\n",
        "# Розподіл позицій/груп\n",
        "df[\"primary_position\"].value_counts(dropna=False).to_frame(\"n\").to_csv(OUT_T/\"EDA_pos_count.csv\")\n",
        "df[\"role_group\"].value_counts(dropna=False).to_frame(\"n\").to_csv(OUT_T/\"EDA_role_group_count.csv\")\n",
        "\n",
        "# Контамінація ознак GK vs польові\n",
        "GK_COLS = [c for c in df.columns if c.startswith(\"goalkeeping_\")]\n",
        "FIELD_GROUP_PREFIXES = [\"attacking_\",\"skill_\",\"movement_\",\"power_\",\"mentality_\",\"defending_\"]\n",
        "FIELD_DET = [c for c in df.columns if any(c.startswith(p) for p in FIELD_GROUP_PREFIXES)]\n",
        "contam = {}\n",
        "if GK_COLS:\n",
        "    non_gk = df[df[\"role_group\"]!=\"GK\"]\n",
        "    gk = df[df[\"role_group\"]==\"GK\"]\n",
        "    contam[\"nonGK_has_GK_notnull_frac\"] = float(non_gk[GK_COLS].notna().mean().mean())\n",
        "    contam[\"GK_has_FIELD_notnull_frac\"] = float(gk[FIELD_DET].notna().mean().mean()) if FIELD_DET else np.nan\n",
        "pd.DataFrame([contam]).to_csv(OUT_T/\"EDA_contamination.csv\", index=False)\n",
        "\n",
        "# Кореляції ядра й сильні пари\n",
        "core = [c for c in [\"pace\",\"shooting\",\"passing\",\"dribbling\",\"defending\",\"physic\",\"overall\"] if c in df.columns]\n",
        "strong_pairs = corr_pairs(df, core, thr=0.9) if len(core)>=2 else []\n",
        "pd.DataFrame(strong_pairs, columns=[\"f1\",\"f2\",\"r\"]).to_csv(OUT_T/\"EDA_core_strong_corr.csv\", index=False)\n",
        "\n",
        "# Потенційні leakage-фічі (списком)\n",
        "leak_like = [c for c in df.columns if c in [\"club_position\",\"nation_position\",\"club_jersey_number\",\"nation_jersey_number\"]]\n",
        "pd.Series(leak_like, name=\"leak_like\").to_csv(OUT_T/\"EDA_potential_leakage.csv\", index=False)\n",
        "\n",
        "# Повні сильні кореляції (Pearson/Spearman) — виклик нашої утиліти\n",
        "save_strong_corr(df, OUT_T, thr=0.90)\n",
        "\n",
        "print(\"[EDA] OK. Збережено таблиці в out/tables/ (EDA_*)\")\n",
        "\n",
        "#  18) Feature Engineering\n",
        "# Мапи цілі\n",
        "df[\"target_pos15\"] = df[\"primary_position\"].str.upper()\n",
        "df[\"target_grp4\"]  = df[\"role_group\"]\n",
        "\n",
        "# Індекси-агрегати (беремо що є)\n",
        "PACE = [c for c in [\"movement_acceleration\",\"movement_sprint_speed\",\"pace\"] if c in df.columns]\n",
        "DRIB = [c for c in [\"dribbling\",\"skill_dribbling\",\"skill_ball_control\",\"movement_agility\",\"movement_balance\"] if c in df.columns]\n",
        "PASS = [c for c in [\"passing\",\"attacking_short_passing\",\"skill_long_passing\",\"mentality_vision\",\"skill_curve\",\"attacking_crossing\"] if c in df.columns]\n",
        "SHOOT = [c for c in [\"shooting\",\"attacking_finishing\",\"attacking_volleys\",\"power_shot_power\",\"power_long_shots\",\"attacking_heading_accuracy\",\"mentality_penalties\"] if c in df.columns]\n",
        "DEFND = [c for c in [\"defending\",\"defending_marking_awareness\",\"defending_standing_tackle\",\"defending_sliding_tackle\",\"mentality_interceptions\",\"power_strength\"] if c in df.columns]\n",
        "PHYS = [c for c in [\"physic\",\"power_stamina\",\"power_strength\",\"power_jumping\"] if c in df.columns]\n",
        "GKSET = [c for c in [\"goalkeeping_diving\",\"goalkeeping_reflexes\",\"goalkeeping_positioning\",\"goalkeeping_handling\",\"goalkeeping_kicking\",\"goalkeeping_speed\"] if c in df.columns]\n",
        "\n",
        "for grp, cols in [(\"pace_idx\", PACE),(\"dribble_idx\", DRIB),(\"playmake_idx\", PASS),\n",
        "                  (\"attack_idx\", SHOOT),(\"defend_idx\", DEFND),(\"phys_idx\", PHYS),(\"gk_idx\", GKSET)]:\n",
        "    if cols:\n",
        "        df[grp] = df[cols].mean(axis=1, skipna=True)\n",
        "\n",
        "# BMI, age^2\n",
        "if \"height_cm\" in df.columns and \"weight_kg\" in df.columns:\n",
        "    h_m = df[\"height_cm\"]/100.0\n",
        "    df[\"bmi\"] = df[\"weight_kg\"]/(h_m*h_m)\n",
        "if \"age\" in df.columns:\n",
        "    df[\"age2\"] = df[\"age\"]**2\n",
        "\n",
        "# work_rate → два числових\n",
        "if \"work_rate\" in df.columns:\n",
        "    wr = df[\"work_rate\"].apply(parse_work_rate)\n",
        "    df[\"work_rate_att\"] = wr.apply(lambda t: t[0])\n",
        "    df[\"work_rate_def\"] = wr.apply(lambda t: t[1])\n",
        "\n",
        "# preferred_foot → is_left\n",
        "if \"preferred_foot\" in df.columns:\n",
        "    df[\"is_left\"] = (df[\"preferred_foot\"].str.upper()==\"LEFT\").astype(\"Int64\")\n",
        "\n",
        "# Легка імпутація числових: медіана за role_group → глобальна\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_cols = [c for c in num_cols if c not in [\"sofifa_id\"]]  # захист id\n",
        "med_by_role = df.groupby(\"role_group\")[num_cols].median(numeric_only=True)\n",
        "def impute_row(row):\n",
        "    rg = row[\"role_group\"]\n",
        "    for c in num_cols:\n",
        "        if pd.isna(row[c]):\n",
        "            if rg in med_by_role.index and pd.notna(med_by_role.loc[rg, c]):\n",
        "                row[c] = med_by_role.loc[rg, c]\n",
        "    return row\n",
        "df_num = df.copy()\n",
        "df_num[num_cols] = df_num[num_cols].astype(float)\n",
        "df_num = df_num.apply(impute_row, axis=1)\n",
        "for c in num_cols:\n",
        "    df_num[c] = df_num[c].fillna(df_num[c].median())  # остаточний fallback\n",
        "\n",
        "# Вибір «безпечних» фіч (без ідентифікаторів/URL/ліків)\n",
        "drop_like = set([\n",
        "    \"player_url\",\"long_name\",\"short_name\",\"club_logo_url\",\"nation_logo_url\",\"nation_flag_url\",\n",
        "    \"club_position\",\"nation_position\",\"club_jersey_number\",\"nation_jersey_number\"\n",
        "])\n",
        "keep_num = [c for c in num_cols if c not in drop_like]\n",
        "keep_cat = [c for c in [\"preferred_foot\",\"work_rate\",\"body_type\",\"league_name\",\"club_name\",\"nationality_name\"] if c in df.columns]\n",
        "\n",
        "fe_base = df_num[[\"sofifa_id\",\"primary_position\",\"target_pos15\",\"target_grp4\"] + keep_num + keep_cat]\n",
        "fe_base.to_csv(OUT_T/\"Task18_features_base.csv\", index=False)\n",
        "\n",
        "print(\"[FE] OK. Збережено базову матрицю ознак: Task18_features_base.csv\")\n",
        "\n",
        "#  19) Feature Selection\n",
        "# Мішень: мультиклас за target_pos15 (лише канонічні)\n",
        "fe = fe_base[fe_base[\"target_pos15\"].isin(CANON15)].copy()\n",
        "y = fe[\"target_pos15\"].astype(str)\n",
        "\n",
        "# Підготовка X: числові + one-hot категорій\n",
        "X_num = fe[keep_num].copy()\n",
        "if keep_cat:\n",
        "    ohe = OHE_dense()\n",
        "    X_cat_arr = ohe.fit_transform(fe[keep_cat].astype(str))\n",
        "    ohe_cols = ohe.get_feature_names_out(keep_cat).tolist()\n",
        "    X_cat_df = pd.DataFrame(X_cat_arr, columns=ohe_cols, index=fe.index)\n",
        "    X_full = pd.concat([X_num, X_cat_df], axis=1)\n",
        "else:\n",
        "    X_full = X_num.copy()\n",
        "    ohe_cols = []\n",
        "\n",
        "# Видалення надсильних кореляцій (|r|≥0.95) серед числових\n",
        "strong = corr_pairs(X_num, X_num.columns.tolist(), thr=0.95)\n",
        "drop_corr = set([b for (a,b,_) in strong])\n",
        "X_filt = X_full.drop(columns=list(drop_corr), errors=\"ignore\")\n",
        "\n",
        "# MI (фільтраційно)\n",
        "mi = mutual_info_classif(X_filt.fillna(0), y, discrete_features=[c in ohe_cols for c in X_filt.columns], random_state=42)\n",
        "mi_tbl = pd.DataFrame({\"feature\": X_filt.columns, \"mi\": mi}).sort_values(\"mi\", ascending=False)\n",
        "mi_tbl.to_csv(OUT_T/\"Task19_mi_features_pos15.csv\", index=False)\n",
        "\n",
        "# RF (вбудоване)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "rf.fit(X_filt.fillna(0), y)\n",
        "rf_imp = pd.DataFrame({\"feature\": X_filt.columns, \"rf_importance\": rf.feature_importances_}) \\\n",
        "         .sort_values(\"rf_importance\", ascending=False)\n",
        "rf_imp.to_csv(OUT_T/\"Task19_rf_importance_pos15.csv\", index=False)\n",
        "\n",
        "# Рекомендований набір: перетин топ-N (MI, RF), без корельованих\n",
        "TOPN = 60\n",
        "top_mi = set(mi_tbl.head(TOPN)[\"feature\"])\n",
        "top_rf = set(rf_imp.head(TOPN)[\"feature\"])\n",
        "sel = sorted((top_mi & top_rf) - drop_corr)\n",
        "\n",
        "pd.Series(sel, name=\"selected_feature\").to_csv(OUT_T/\"Task19_selected_features_pos15.csv\", index=False)\n",
        "\n",
        "print(\"[FS] OK. MI/RF/selected збережені в out/tables/. Рекомендовано використати Task19_selected_features_pos15.csv\")\n"
      ],
      "metadata": {
        "id": "qxoSA5rT9AER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета. Виявити гравців із підозріло малим/великим зростом/вагою всередині ролі (GK/DEF/MID/FWD) і гравців без цих показників.\n",
        "Що зберігаємо.\n",
        "\n",
        "Task17_role_extremes_height_weight.csv — 1-й і 99-й перцентил за роллю; все, що ≤1% або ≥99% → прапоримо.\n",
        "\n",
        "Task17_missing_bodymetrics.csv — усі, у кого бракує height/weight.\n",
        "Тлумачення. Це не «помилка», а екран: вибірка для ручної перевірки або окремого бізнес-правила (наприклад, дропнути екстремальних GK зі зростом < 170 см, якщо це шкодить моделі)."
      ],
      "metadata": {
        "id": "qgJn8YHtkQ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task17.A: Екстремальні/відсутні антропометрії (за role_group)\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# налаштування порогів (ранговий підхід: 1% знизу + 1% згори)\n",
        "P_EXT_FRAC     = 0.01\n",
        "MIN_N_PER_ROLE = 30\n",
        "\n",
        "# гарантуємо наявність role_group / primary_position\n",
        "DEF = {\"CB\",\"LB\",\"RB\",\"LWB\",\"RWB\"}\n",
        "MID = {\"CDM\",\"CM\",\"CAM\",\"LM\",\"RM\"}\n",
        "FWD = {\"ST\",\"CF\",\"LW\",\"RW\"}\n",
        "\n",
        "def make_primary_position(s: pd.Series) -> pd.Series:\n",
        "    # перша позиція як primary\n",
        "    return s.astype(str).str.split(\",\", n=1, expand=True)[0].str.strip()\n",
        "\n",
        "def role_group_fn(pos: str) -> str:\n",
        "    p = str(pos).upper()\n",
        "    if p == \"GK\": return \"GK\"\n",
        "    if p in DEF:  return \"DEF\"\n",
        "    if p in MID:  return \"MID\"\n",
        "    if p in FWD:  return \"FWD\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "if \"primary_position\" not in df.columns and \"player_positions\" in df.columns:\n",
        "    df[\"primary_position\"] = make_primary_position(df[\"player_positions\"])\n",
        "if \"role_group\" not in df.columns and \"primary_position\" in df.columns:\n",
        "    df[\"role_group\"] = df[\"primary_position\"].apply(role_group_fn)\n",
        "\n",
        "# вибираємо доступну назву гравця\n",
        "NAME_COL = \"short_name\" if \"short_name\" in df.columns else (\"long_name\" if \"long_name\" in df.columns else None)\n",
        "\n",
        "# цільові колонки на виході\n",
        "COLS_OUT = [\"sofifa_id\",\"name\",\"primary_position\",\"role_group\",\"col\",\"value\"]\n",
        "\n",
        "ext_frames, miss_frames = [], []\n",
        "\n",
        "for grp, part in df.groupby(\"role_group\"):\n",
        "    if grp not in {\"GK\",\"DEF\",\"MID\",\"FWD\"} or len(part) < MIN_N_PER_ROLE:\n",
        "        continue\n",
        "\n",
        "    for col in [\"height_cm\",\"weight_kg\"]:\n",
        "        if col not in part.columns:\n",
        "            continue\n",
        "\n",
        "        # відсутні значення\n",
        "        miss_mask = part[col].isna()\n",
        "        if miss_mask.any():\n",
        "            base = pd.DataFrame({\n",
        "                \"sofifa_id\": part.loc[miss_mask, \"sofifa_id\"].values if \"sofifa_id\" in part.columns else pd.NA,\n",
        "                \"primary_position\": part.loc[miss_mask, \"primary_position\"].values if \"primary_position\" in part.columns else pd.NA,\n",
        "                \"role_group\": grp,\n",
        "                \"col\": col,\n",
        "                \"value\": np.nan,\n",
        "            })\n",
        "            if NAME_COL and NAME_COL in part.columns:\n",
        "                base[\"name\"] = part.loc[miss_mask, NAME_COL].values\n",
        "            else:\n",
        "                # заглушка з ID\n",
        "                base[\"name\"] = base[\"sofifa_id\"].apply(lambda x: f\"player_{int(x)}\" if pd.notna(x) else \"player_unknown\")\n",
        "            miss_frames.append(base[COLS_OUT])\n",
        "\n",
        "        # екстреми рангово (1% low + 1% high)\n",
        "        p = part.dropna(subset=[col])\n",
        "        if p.empty:\n",
        "            continue\n",
        "        n_each = max(1, int(np.ceil(P_EXT_FRAC * len(p))))\n",
        "        s = p.sort_values(col)\n",
        "        low  = s.head(n_each)\n",
        "        high = s.tail(n_each)\n",
        "        both = pd.concat([low, high], ignore_index=False)\n",
        "\n",
        "        if not both.empty:\n",
        "            base = pd.DataFrame({\n",
        "                \"sofifa_id\": both[\"sofifa_id\"].values if \"sofifa_id\" in both.columns else pd.NA,\n",
        "                \"primary_position\": both[\"primary_position\"].values if \"primary_position\" in both.columns else pd.NA,\n",
        "                \"role_group\": grp,\n",
        "                \"col\": col,\n",
        "                \"value\": both[col].values,\n",
        "            })\n",
        "            if NAME_COL and NAME_COL in both.columns:\n",
        "                base[\"name\"] = both[NAME_COL].values\n",
        "            else:\n",
        "                base[\"name\"] = base[\"sofifa_id\"].apply(lambda x: f\"player_{int(x)}\" if pd.notna(x) else \"player_unknown\")\n",
        "            ext_frames.append(base[COLS_OUT])\n",
        "\n",
        "# збірка результатів з заголовками навіть якщо пусто\n",
        "ext_df  = (pd.concat(ext_frames, ignore_index=True)  if ext_frames else pd.DataFrame(columns=COLS_OUT))\n",
        "miss_df = (pd.concat(miss_frames, ignore_index=True) if miss_frames else pd.DataFrame(columns=COLS_OUT))\n",
        "\n",
        "ext_df.to_csv(OUT_T/\"Task17_role_extremes_height_weight.csv\", index=False)\n",
        "miss_df.to_csv(OUT_T/\"Task17_missing_bodymetrics.csv\", index=False)\n",
        "\n",
        "print(f\"[A] extremes rows: {len(ext_df)} | missing rows: {len(miss_df)} \"\n",
        "      f\"(frac={P_EXT_FRAC}, min_n={MIN_N_PER_ROLE}, name_col={NAME_COL or 'placeholder'})\")\n"
      ],
      "metadata": {
        "id": "-iJnWlLFAyz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета. Відловити гравців, чия ціна або зарплата нетипова для їх overall, віку та ліги (контроль за контекстом!).\n",
        "Що зберігаємо.\n",
        "\n",
        "Task17_value_outliers_byrole.csv, Task17_wage_outliers_byrole.csv — спостереження з |z-resid| ≥ Z_THR.\n",
        "Метод. Для кожної ролі: регресуємо log1p(value/wage) ~ overall + age + OHE(league_topN), беремо стандартизований залишок і відсікаємо хвости."
      ],
      "metadata": {
        "id": "pbXNTnGrM19B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task17.B: Аномалії value/wage за ролями (лінійна модель + Z-резидуали)\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "Z_THR = 4.0  # чутливість (менше → більше аномалій)\n",
        "TOP_LEAG = df[\"league_name\"].value_counts().head(30).index if \"league_name\" in df.columns else []\n",
        "\n",
        "def ohe_league(s: pd.Series) -> pd.Series:\n",
        "    return s.where(s.isin(TOP_LEAG), \"Other\").astype(str)\n",
        "\n",
        "def OHE_dense():\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def role_linear_outliers(data: pd.DataFrame, y_col: str, z_thr: float = Z_THR) -> pd.DataFrame:\n",
        "    cols_out = [\"sofifa_id\",\"short_name\",\"club_name\",\"league_name\",\n",
        "                \"primary_position\",\"role_group\",\"overall\",\"age\", y_col, \"resid_z_log\"]\n",
        "    if data.empty:\n",
        "        return pd.DataFrame(columns=cols_out)\n",
        "\n",
        "    need_cols = [y_col,\"overall\",\"age\",\"role_group\",\"primary_position\"]\n",
        "    for c in need_cols:\n",
        "        if c not in data.columns:\n",
        "            return pd.DataFrame(columns=cols_out)\n",
        "\n",
        "    part0 = data.dropna(subset=[y_col,\"overall\",\"age\"]).copy()\n",
        "    if part0.empty:\n",
        "        return pd.DataFrame(columns=cols_out)\n",
        "\n",
        "    rows = []\n",
        "    for grp, part in part0.groupby(\"role_group\"):\n",
        "        if len(part) < 10:\n",
        "            continue\n",
        "\n",
        "        # X: overall, age (+ one-hot league)\n",
        "        X_num = part[[\"overall\",\"age\"]].values\n",
        "        if \"league_name\" in part.columns:\n",
        "            Xb = part[[\"league_name\"]].copy()\n",
        "            Xb[\"league_name\"] = ohe_league(Xb[\"league_name\"])\n",
        "            ohe = OHE_dense()\n",
        "            X_cat = ohe.fit_transform(Xb[[\"league_name\"]])\n",
        "            X = np.hstack([X_num, X_cat])\n",
        "        else:\n",
        "            X = X_num\n",
        "\n",
        "        # y: log1p(value/wage)\n",
        "        y = np.log1p(part[y_col].clip(lower=1))\n",
        "        reg = LinearRegression().fit(X, y)\n",
        "        resid = y - reg.predict(X)\n",
        "        s = resid.std(ddof=1)\n",
        "        z = (resid - resid.mean()) / (s if s > 0 else 1.0)\n",
        "\n",
        "        mask = np.abs(z) >= z_thr\n",
        "        if mask.any():\n",
        "            tmp = part.loc[mask, [\"sofifa_id\",\"short_name\",\"club_name\",\"league_name\",\n",
        "                                  \"primary_position\",\"role_group\",\"overall\",\"age\", y_col]].copy()\n",
        "            tmp[\"resid_z_log\"] = z[mask]\n",
        "            rows.append(tmp)\n",
        "\n",
        "    out = (pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=cols_out))\n",
        "    return out[cols_out]\n",
        "\n",
        "# фільтруємо лише додатні значення (лог-трансформація)\n",
        "val_in = df[df.get(\"value_eur\", 0) > 0]\n",
        "wag_in = df[df.get(\"wage_eur\", 0)  > 0]\n",
        "\n",
        "val_out = role_linear_outliers(val_in, \"value_eur\", z_thr=Z_THR)\n",
        "wag_out = role_linear_outliers(wag_in, \"wage_eur\",  z_thr=Z_THR)\n",
        "\n",
        "val_out.to_csv(OUT_T/\"Task17_value_outliers_byrole.csv\", index=False)\n",
        "wag_out.to_csv(OUT_T/\"Task17_wage_outliers_byrole.csv\",  index=False)\n",
        "\n",
        "print(f\"[B] value_outliers: {len(val_out)} | wage_outliers: {len(wag_out)} | z_thr={Z_THR}, top_leagues={len(TOP_LEAG)}\")\n"
      ],
      "metadata": {
        "id": "zwQ8c-GsA22b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета. Знайти гравців у топ-квантілі overall всередині своєї ролі, у яких ключові скіли для цієї ролі нижчі за рольові пороги.\n",
        "Що зберігаємо.\n",
        "\n",
        "Task17_high_overall_but_weak_core_attrs.csv — підозрілі записи для ручної перевірки.\n",
        "Правила (можна підкрутити під курс/захист):\n",
        "\n",
        "беремо верхні 10% за overall у ролі;\n",
        "\n",
        "«низькі обидва» — скіли нижче 20-го перцентиля ролі;\n",
        "\n",
        "«дуже низький один» — хоча б один нижче 10-го перцентиля."
      ],
      "metadata": {
        "id": "xOlpfWKtNFuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task17.C: High overall but weak core skills (screen)\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "P_TOP_OVERALL   = 0.90\n",
        "P_BOTH_LOW      = 0.20\n",
        "P_VLOW_ONE      = 0.10\n",
        "MIN_N_PER_ROLE  = 50\n",
        "\n",
        "role_rules = {\n",
        "    \"FWD\": {\"low_both\": [\"attack_idx\",\"pace_idx\"],      \"vlow_one\": [\"attack_idx\",\"pace_idx\"]},\n",
        "    \"MID\": {\"low_both\": [\"playmake_idx\",\"dribble_idx\"], \"vlow_one\": [\"playmake_idx\"]},\n",
        "    \"DEF\": {\"low_both\": [\"defend_idx\",\"phys_idx\"],      \"vlow_one\": [\"defend_idx\"]},\n",
        "    \"GK\":  {\"low_both\": [\"gk_idx\"],                     \"vlow_one\": [\"gk_idx\"]},\n",
        "}\n",
        "\n",
        "# вибираємо доступну назву гравця (для красивого виводу)\n",
        "NAME_COL = \"short_name\" if \"short_name\" in df.columns else (\"long_name\" if \"long_name\" in df.columns else None)\n",
        "\n",
        "rows = []\n",
        "for grp, part in df.groupby(\"role_group\"):\n",
        "    # фільтр мін.розміру і наявності overall\n",
        "    if grp not in role_rules or len(part) < MIN_N_PER_ROLE or \"overall\" not in part.columns:\n",
        "        continue\n",
        "    rr = role_rules[grp]\n",
        "\n",
        "    # кандидати: верхній квантіль за overall усередині ролі\n",
        "    part_ok = part[part[\"overall\"].notna()].copy()\n",
        "    if part_ok.empty:\n",
        "        continue\n",
        "    top_over = part_ok[\"overall\"].quantile(P_TOP_OVERALL)\n",
        "    cand = part_ok[part_ok[\"overall\"] >= top_over].copy()\n",
        "    if cand.empty:\n",
        "        continue\n",
        "\n",
        "    # «обидва низькі» — лише за тими колонками, що реально існують\n",
        "    low_both_cols = [c for c in rr[\"low_both\"] if c in part_ok.columns]\n",
        "    if low_both_cols:\n",
        "        both_low = np.ones(len(cand), dtype=bool)\n",
        "        for c in low_both_cols:\n",
        "            thr = part_ok[c].quantile(P_BOTH_LOW)\n",
        "            both_low &= cand[c] < thr\n",
        "    else:\n",
        "        both_low = np.zeros(len(cand), dtype=bool)\n",
        "\n",
        "    # «дуже низький один»\n",
        "    vlow_cols = [c for c in rr[\"vlow_one\"] if c in part_ok.columns]\n",
        "    vlow = np.zeros(len(cand), dtype=bool)\n",
        "    for c in vlow_cols:\n",
        "        thr = part_ok[c].quantile(P_VLOW_ONE)\n",
        "        vlow |= cand[c] < thr\n",
        "\n",
        "    # формуємо підсумкову таблицю лише з наявними колонками\n",
        "    base_cols = [\"sofifa_id\",\"primary_position\",\"overall\"]\n",
        "    if \"club_name\" in cand.columns: base_cols.insert(1, \"club_name\")\n",
        "    if NAME_COL and NAME_COL in cand.columns: base_cols.insert(1, NAME_COL)\n",
        "\n",
        "    skill_cols_present = list(set(low_both_cols + vlow_cols) & set(cand.columns))\n",
        "    sus = cand.loc[both_low | vlow, base_cols + skill_cols_present].copy()\n",
        "    if not sus.empty:\n",
        "        sus[\"role_group\"] = grp\n",
        "        rows.append(sus)\n",
        "\n",
        "# збірка і збереження (навіть якщо порожньо — з заголовками)\n",
        "all_cols = sorted(set().union(*(t.columns for t in rows))) if rows else base_cols + [\"role_group\"]\n",
        "out_c = (pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=all_cols))\n",
        "out_c.to_csv(OUT_T/\"Task17_high_overall_but_weak_core_attrs.csv\", index=False)\n",
        "\n",
        "print(f\"[C] weak-core among top overall: {len(out_c)}\")\n"
      ],
      "metadata": {
        "id": "Korq0lxHA7A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета. Побудувати чистий train-набір для майбутнього моделювання позиції:\n",
        "– нормалізувати позиції → лише канонічні 15;\n",
        "– вимагати достатність ключових індексів за роллю;\n",
        "– виключити жорсткі фінансові аутлайєри;\n",
        "– зберегти Task18_features_train.csv.\n",
        "Що зберігаємо.\n",
        "\n",
        "Task18_features_train.csv — підмножина Task18_features_base.csv із валідними sofifa_id."
      ],
      "metadata": {
        "id": "Ht1Ap88tNO87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task18: Маска якості → features_train\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# гарантуємо primary_position / role_group\n",
        "DEF = {\"CB\",\"LB\",\"RB\",\"LWB\",\"RWB\"}\n",
        "MID = {\"CDM\",\"CM\",\"CAM\",\"LM\",\"RM\"}\n",
        "FWD = {\"ST\",\"CF\",\"LW\",\"RW\"}\n",
        "\n",
        "def make_primary_position(s: pd.Series) -> pd.Series:\n",
        "    # перша позиція як primary\n",
        "    return s.astype(str).str.split(\",\", n=1, expand=True)[0].str.strip()\n",
        "\n",
        "def role_group_fn(pos: str) -> str:\n",
        "    p = str(pos).upper()\n",
        "    if p == \"GK\": return \"GK\"\n",
        "    if p in DEF:  return \"DEF\"\n",
        "    if p in MID:  return \"MID\"\n",
        "    if p in FWD:  return \"FWD\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "if \"primary_position\" not in df.columns and \"player_positions\" in df.columns:\n",
        "    df[\"primary_position\"] = make_primary_position(df[\"player_positions\"])\n",
        "\n",
        "# 🔧 нормалізація: робимо UPPERCASE для консистентності (без жодних мап SK→ST)\n",
        "df[\"primary_position\"] = df[\"primary_position\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "if \"role_group\" not in df.columns and \"primary_position\" in df.columns:\n",
        "    df[\"role_group\"] = df[\"primary_position\"].apply(role_group_fn)\n",
        "\n",
        "# канонічні позиції\n",
        "CANON15 = {\"GK\",\"CB\",\"LB\",\"RB\",\"LWB\",\"RWB\",\"CDM\",\"CM\",\"CAM\",\"LM\",\"RM\",\"LW\",\"RW\",\"ST\",\"CF\"}\n",
        "mask_pos = df[\"primary_position\"].isin(CANON15)\n",
        "\n",
        "# заповненість ключових індексів (правило: ≥ половини з переліку)\n",
        "need = {\n",
        "    \"FWD\": [\"attack_idx\",\"pace_idx\",\"dribble_idx\"],\n",
        "    \"MID\": [\"playmake_idx\",\"dribble_idx\"],\n",
        "    \"DEF\": [\"defend_idx\",\"phys_idx\",\"pace_idx\"],\n",
        "    \"GK\":  [\"gk_idx\"]\n",
        "}\n",
        "def enough_cols(row: pd.Series) -> bool:\n",
        "    cols = need.get(row.get(\"role_group\"), [])\n",
        "    if not cols:\n",
        "        return True\n",
        "    ok = [pd.notna(row[c]) for c in cols if c in row.index]\n",
        "    return (sum(ok) >= max(1, int(np.ceil(len(cols)/2))))\n",
        "\n",
        "mask_enough = df.apply(enough_cols, axis=1)\n",
        "\n",
        "# жорсткі аномалії (ID із Task17.B)\n",
        "def load_ids(rel_path: str) -> set:\n",
        "    p = OUT_T / rel_path\n",
        "    if not p.exists():\n",
        "        return set()\n",
        "    try:\n",
        "        t = pd.read_csv(p)\n",
        "        return set(t[\"sofifa_id\"].dropna().astype(int))\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "bad_ids = load_ids(\"Task17_value_outliers_byrole.csv\") | load_ids(\"Task17_wage_outliers_byrole.csv\")\n",
        "mask_anom = ~df[\"sofifa_id\"].astype(int).isin(bad_ids)\n",
        "\n",
        "# все разом\n",
        "train_mask = (mask_pos & mask_enough & mask_anom &\n",
        "              df[\"overall\"].notna() & df[\"age\"].notna())\n",
        "\n",
        "# читання базових фіч\n",
        "FE_BASE_PATH = OUT_T / \"Task18_features_base.csv\"\n",
        "if not FE_BASE_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Не знайдено {FE_BASE_PATH}. Спочатку збережи базову матрицю ознак (Task18_features_base.csv).\")\n",
        "\n",
        "fe_base = pd.read_csv(FE_BASE_PATH, low_memory=False)\n",
        "fe_train = fe_base[fe_base[\"sofifa_id\"].isin(df.loc[train_mask, \"sofifa_id\"])]\n",
        "\n",
        "fe_train.to_csv(OUT_T / \"Task18_features_train.csv\", index=False)\n",
        "print(f\"[TRAIN] {fe_train.shape[0]} рядків у Task18_features_train.csv (з {fe_base.shape[0]})\")\n"
      ],
      "metadata": {
        "id": "HkU3oxQgCuEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета блоку\n",
        "\n",
        "Що робимо: готуємо два варіанти ознак для задачі класифікації позиції та відбираємо найінформативніші змінні.\n",
        "\n",
        "Навіщо два варіанти:\n",
        "A) компактні індекси (інтерпретованість, низька мультиколінеарність),\n",
        "B) детальні атрибути (вища гнучкість моделей).\n",
        "\n",
        "Методологія відбору: комбінуємо ранги Mutual Information (нелінійний зв’язок із таргетом) та RandomForest importance (середнє по крос-валідації). Отримаємо об’єднаний рейтинг і виберемо топ-K."
      ],
      "metadata": {
        "id": "NP8OHgCYE5sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task18: формування фічсетів A/B\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "IN = Path(\"out/tables/Task18_features_train.csv\")\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(IN, low_memory=False)\n",
        "\n",
        "# -- невеликі утиліти ---------------------------------------------------------\n",
        "def has(col): return col in df.columns\n",
        "def safe_cols(cands): return [c for c in cands if has(c)]\n",
        "\n",
        "# робимо числові лог-версії вартості/зарплати (можуть знадобитися пізніше)\n",
        "for c in [\"value_eur\",\"wage_eur\"]:\n",
        "    if has(c):\n",
        "        df[f\"log1p_{c}\"] = np.log1p(pd.to_numeric(df[c], errors=\"coerce\"))\n",
        "\n",
        "# розбір work_rate \"High/ Medium\" -> дві числові (лише якщо нема)\n",
        "if has(\"work_rate\") and (not has(\"work_att\") or not has(\"work_def\")):\n",
        "    m = df[\"work_rate\"].astype(str).str.lower().str.extract(r\"(low|medium|high)[\\s/]*(low|medium|high)?\")\n",
        "    map3 = {\"low\":0, \"medium\":1, \"high\":2}\n",
        "    df[\"work_att\"] = df.get(\"work_att\", m[0].map(map3))\n",
        "    df[\"work_def\"] = df.get(\"work_def\", m[1].map(map3))\n",
        "\n",
        "# ---- таргет\n",
        "y_col = \"primary_position\"\n",
        "if y_col not in df.columns:\n",
        "    raise ValueError(\"Немає стовпця primary_position у Task18_features_train.csv\")\n",
        "\n",
        "# ---- фічсет A: індекси + базова антропометрія/профіль\n",
        "IDX  = safe_cols([\"pace_idx\",\"dribble_idx\",\"playmake_idx\",\"attack_idx\",\"defend_idx\",\"phys_idx\",\"gk_idx\"])\n",
        "BASE = safe_cols([\"overall\",\"potential\",\"age\",\"height_cm\",\"weight_kg\",\"weak_foot\",\"skill_moves\",\"international_reputation\"])\n",
        "CATS = safe_cols([\"preferred_foot\",\"body_type\"]) + safe_cols([\"work_rate\"])  # сирі категорії (для one-hot у пайплайні)\n",
        "WR   = safe_cols([\"work_att\",\"work_def\"])  # числовий розбір\n",
        "\n",
        "FEAT_A = list(dict.fromkeys(IDX + BASE + WR + CATS))  # порядок без дублів\n",
        "\n",
        "# ---- фічсет B: детальні навички (усі доступні «технічні» стовпці)\n",
        "PATTERNS = (\"attacking_\",\"skill_\",\"movement_\",\"mentality_\",\"power_\",\"defending_\",\"goalkeeping_\")\n",
        "DETAIL = [c for c in df.columns if c.startswith(PATTERNS)]\n",
        "FEAT_B = list(dict.fromkeys(DETAIL + BASE + WR + CATS))\n",
        "\n",
        "# діагностика: чи є кандидати, яких немає у df (на випадок майбутніх рефакторингів)\n",
        "missing_A = [c for c in (IDX + BASE + WR + CATS) if c not in df.columns]\n",
        "missing_B = [c for c in (DETAIL + BASE + WR + CATS) if c not in df.columns]\n",
        "if missing_A:\n",
        "    print(f\"[WARN] Відсутні у df кандидати з набору A: {missing_A}\")\n",
        "if missing_B:\n",
        "    print(f\"[WARN] Відсутні у df кандидати з набору B: {missing_B[:10]}{' ...' if len(missing_B)>10 else ''}\")\n",
        "\n",
        "# збережемо метадані\n",
        "pd.Series(FEAT_A, name=\"feature\").to_csv(OUT_T/\"Task18_featset_A_list.csv\", index=False)\n",
        "pd.Series(FEAT_B, name=\"feature\").to_csv(OUT_T/\"Task18_featset_B_list.csv\", index=False)\n",
        "\n",
        "print(f\"[A] {len(FEAT_A)} ознак;  [B] {len(FEAT_B)} ознак\")\n"
      ],
      "metadata": {
        "id": "v2u8V06WE_ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета. Автоматично відібрати найінформативніші фічі для мультикласової цілі primary_position, щоб надалі тренувати компактні й стійкі моделі.\n",
        "\n",
        "Як працює блок:\n",
        "\n",
        "Читаємо тренувальну матрицю з Task18_features_train.csv. Якщо немає числових work_att/work_def, відновлюємо їх із текстового work_rate.\n",
        "\n",
        "Для кожного списку фіч A/B (збережені в Task18_featset_A_list.csv / Task18_featset_B_list.csv) виконуємо:\n",
        "\n",
        "Підготовку даних: медіанна імпутація числових, one-hot для категоріальних (щільна матриця; сумісно зі старими/новими версіями sklearn).\n",
        "\n",
        "Filter-крок (MI): рахуємо mutual_info_classif між ознаками та ціллю; для категоріальних агрегуємо важливість назад на “сиру” фічу (максимум по її one-hot стовпцях).\n",
        "\n",
        "Embedded-крок (RF): 5-фолдова StratifiedKFold з RandomForestClassifier; усереднюємо важливості та також згортаємо їх на рівень вихідних фіч.\n",
        "\n",
        "Зведений рейтинг: ранжуємо за MI та RF, складаємо бали (rank_sum) і сортуємо.\n",
        "\n",
        "Анти-колінеарність: для числових дропаємо надлишково корельовані пари (|r| ≥ 0.95), лишаючи одну з них.\n",
        "\n",
        "Вибір: беремо top-k (для A – 40, для B – 60).\n",
        "\n",
        "Результати зберігаються у:\n",
        "\n",
        "Task19_feature_ranks_A.csv, Task19_selected_features_A.csv\n",
        "\n",
        "Task19_feature_ranks_B.csv, Task19_selected_features_B.csv\n",
        "\n",
        "Чому саме так:\n",
        "\n",
        "MI ловить нелінійні та не монотонні залежності; RF враховує взаємодії та є стійким до шуму.\n",
        "\n",
        "CV зменшує варіативність оцінок, агрегація one-hot не дає категоріальним фічам “перемножувати” свій вплив, кореляційний фільтр прибирає мультиколінеарність.\n",
        "\n",
        "class_weight=\"balanced_subsample\" допомагає при дисбалансі класів."
      ],
      "metadata": {
        "id": "LQaQbJ_6fjnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Task19: авто-відбір фіч (MI + RandomForest, CV) — СУМІСНИЙ ПАТЧ\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "IN = Path(\"out/tables/Task18_features_train.csv\")\n",
        "OUT_T = Path(\"out/tables\"); OUT_T.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(IN, low_memory=False)\n",
        "\n",
        "# — відновлюємо work_att/work_def, якщо їх немає, але є work_rate\n",
        "if (\"work_att\" not in df.columns or \"work_def\" not in df.columns) and (\"work_rate\" in df.columns):\n",
        "    m = df[\"work_rate\"].astype(str).str.lower().str.extract(r\"(low|medium|high)[\\s/]*(low|medium|high)?\")\n",
        "    map3 = {\"low\":0, \"medium\":1, \"high\":2}\n",
        "    df[\"work_att\"] = m[0].map(map3)\n",
        "    df[\"work_def\"] = m[1].map(map3)\n",
        "\n",
        "y = df[\"primary_position\"].astype(str)\n",
        "\n",
        "# — списки фіч\n",
        "FEAT_A = pd.read_csv(OUT_T/\"Task18_featset_A_list.csv\")[\"feature\"].tolist()\n",
        "FEAT_B = pd.read_csv(OUT_T/\"Task18_featset_B_list.csv\")[\"feature\"].tolist()\n",
        "\n",
        "# — сумісний one-hot (dense), працює і в нових, і в старих sklearn\n",
        "def OHE_dense():\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def select_features(FEATS, set_name, top_k=40, corr_drop_thr=0.95, random_state=42):\n",
        "    FEATS = [c for c in FEATS if c in df.columns]\n",
        "    if not FEATS:\n",
        "        raise ValueError(f\"Порожній набір ознак {set_name} після фільтрації за наявністю в df.\")\n",
        "    num = [c for c in FEATS if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cat = [c for c in FEATS if not pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "    #  MI (потрібна щільна матриця; фітуємо тільки на df[FEATS])\n",
        "    transformers_mi = []\n",
        "    if num: transformers_mi.append((\"num\", SimpleImputer(strategy=\"median\"), num))\n",
        "    if cat: transformers_mi.append((\"cat\", OHE_dense(), cat))\n",
        "    if not transformers_mi:\n",
        "        raise ValueError(f\"Немає валідних ознак у наборі {set_name}.\")\n",
        "    pre_mi = ColumnTransformer(transformers_mi, remainder=\"drop\")\n",
        "    X_mi = pre_mi.fit_transform(df[FEATS])  # <-- лише підмножина FEATS\n",
        "\n",
        "    # дискретні фічі = ті, що з one-hot\n",
        "    mi_names = pre_mi.get_feature_names_out()\n",
        "    mi_is_discrete = np.array([name.startswith(\"cat__\") for name in mi_names], dtype=bool)\n",
        "\n",
        "    mi_vals = mutual_info_classif(X_mi, y, discrete_features=mi_is_discrete, random_state=random_state)\n",
        "    mi_tbl  = pd.DataFrame({\"col\": mi_names, \"mi\": mi_vals})\n",
        "\n",
        "    # агрегуємо MI з one-hot до сирих назв\n",
        "    agg_mi = {}\n",
        "    for raw in FEATS:\n",
        "        keys = [n for n in mi_names if n.endswith(f\"__{raw}\") or n.startswith(f\"cat__{raw}_\")]\n",
        "        if keys:\n",
        "            agg_mi[raw] = float(mi_tbl.loc[mi_tbl[\"col\"].isin(keys), \"mi\"].max())\n",
        "    mi_rank = pd.DataFrame({\"feature\": list(agg_mi.keys()), \"mi\": list(agg_mi.values())})\n",
        "\n",
        "    #  RF + CV (важливості усереднюємо та згортаємо по one-hot)\n",
        "    transformers_rf = []\n",
        "    if num: transformers_rf.append((\"num\", SimpleImputer(strategy=\"median\"), num))\n",
        "    if cat: transformers_rf.append((\"cat\", OHE_dense(), cat))\n",
        "    pre_rf = ColumnTransformer(transformers_rf, remainder=\"drop\")\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=400, min_samples_split=4,\n",
        "        random_state=random_state, n_jobs=-1,\n",
        "        class_weight=\"balanced_subsample\"\n",
        "    )\n",
        "    pipe = Pipeline([(\"prep\", pre_rf), (\"rf\", rf)])\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "    imp_accum = {f: 0.0 for f in FEATS}\n",
        "\n",
        "    for tr, te in skf.split(df[FEATS], y):\n",
        "        X_tr, y_tr = df.iloc[tr][FEATS], y.iloc[tr]\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "        oh_names = pipe.named_steps[\"prep\"].get_feature_names_out()\n",
        "        imp_oh   = pipe.named_steps[\"rf\"].feature_importances_\n",
        "        imp_tbl  = pd.DataFrame({\"col\": oh_names, \"imp\": imp_oh})\n",
        "        for raw in FEATS:\n",
        "            keys = [n for n in oh_names if n.endswith(f\"__{raw}\") or n.startswith(f\"cat__{raw}_\")]\n",
        "            if keys:\n",
        "                imp_accum[raw] += float(imp_tbl.loc[imp_tbl[\"col\"].isin(keys), \"imp\"].sum())\n",
        "\n",
        "    for k in imp_accum:\n",
        "        imp_accum[k] /= skf.get_n_splits()\n",
        "    rf_rank = pd.DataFrame({\"feature\": list(imp_accum.keys()), \"rf_importance\": list(imp_accum.values())})\n",
        "\n",
        "    #  Об'єднаний рейтинг\n",
        "    tbl = mi_rank.merge(rf_rank, on=\"feature\", how=\"outer\").fillna(0.0)\n",
        "    tbl[\"rank_mi\"] = tbl[\"mi\"].rank(ascending=False, method=\"average\")\n",
        "    tbl[\"rank_rf\"] = tbl[\"rf_importance\"].rank(ascending=False, method=\"average\")\n",
        "    tbl[\"rank_sum\"] = tbl[\"rank_mi\"] + tbl[\"rank_rf\"]\n",
        "    tbl = tbl.sort_values([\"rank_sum\",\"rank_rf\"], ascending=[True, True]).reset_index(drop=True)\n",
        "\n",
        "    #  Дроп надлишково корельованих числових фіч (> corr_drop_thr) на рівні сирих ознак\n",
        "    keep, num_keep = [], []\n",
        "    corr_mat = df[num].corr().abs() if num else pd.DataFrame()\n",
        "    for f in tbl[\"feature\"]:\n",
        "        if f in num and not corr_mat.empty and f in corr_mat.index:\n",
        "            if any((corr_mat.loc[f, g] > corr_drop_thr) for g in num_keep if g in corr_mat.index):\n",
        "                continue\n",
        "            num_keep.append(f)\n",
        "        keep.append(f)\n",
        "\n",
        "    selected = keep[:top_k] if top_k else keep\n",
        "    tbl.to_csv(OUT_T/f\"Task19_feature_ranks_{set_name}.csv\", index=False)\n",
        "    pd.Series(selected, name=\"feature\").to_csv(OUT_T/f\"Task19_selected_features_{set_name}.csv\", index=False)\n",
        "    print(f\"[{set_name}] обрано {len(selected)} фіч (з {len(FEATS)}), top_k={top_k}\")\n",
        "    return tbl, selected\n",
        "\n",
        "rankA, selA = select_features(FEAT_A, \"A\", top_k=40)\n",
        "rankB, selB = select_features(FEAT_B, \"B\", top_k=60)\n"
      ],
      "metadata": {
        "id": "2gve1yPkFJ5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета блоку: саніті-чек / маніфест артефактів Task17–19\n",
        "\n",
        "Що робить цей блок\n",
        "\n",
        "Перевіряє, що всі очікувані CSV з Task17–19 згенеровані, не порожні та свіжі.\n",
        "\n",
        "Друкує зручний звіт зі статусом для кожного файлу: ✅/❌, розмір, кількість рядків у заголовку (head_rows), час модифікації.\n",
        "\n",
        "Формує зведену таблицю-маніфест _manifest_tasks_17_19.csv у out/tables/.\n",
        "\n",
        "Додатково показує «зайві» CSV у каталозі (не перелічені в EXPECTED) — щоби ми нічого не загубили після рефакторингу.\n",
        "\n",
        "Коли запускати\n",
        "\n",
        "Після завершення всіх кроків Task17 → Task19 (або наприкінці ноутбука).\n",
        "\n",
        "Перед синхронізацією у GitHub / CI, щоб упевнитись, що артефакти повні.\n",
        "\n",
        "Як читати звіт\n",
        "\n",
        "✅ — файл існує і має ненульовий розмір; ❌ — відсутній або порожній.\n",
        "\n",
        "head_rows — скільки рядків прочиталося з head (швидка перевірка, що файл валідний CSV).\n",
        "\n",
        "mtime — коли файл востаннє оновлювався (корисно, якщо підозрюємо «застряглі» артефакти).\n",
        "\n",
        "Гнучкість і розширення\n",
        "\n",
        "Список очікуваних файлів задається у словнику EXPECTED по тасках — просто додайте новий файл до відповідного списку.\n",
        "\n",
        "Прапорець STRICT:\n",
        "\n",
        "False (дефолт): лише друкуємо попередження.\n",
        "\n",
        "True: якщо є відсутні/порожні критичні файли — падаємо з assert (зручно для CI).\n",
        "\n",
        "Типові причини «❌» і як виправити\n",
        "\n",
        "Не запущені попередні комірки (наприклад, Task17.B для outliers).\n",
        "\n",
        "Занадто суворі пороги (наприклад, Z_THR у Task17.B → спробуйте 3.0–3.5).\n",
        "\n",
        "Неправильний шлях виводу (має бути out/tables).\n",
        "\n",
        "Дані фільтрами «вичистилися» (перевірте train_mask, наявність колонок, тощо).\n",
        "\n",
        "Вихід\n",
        "\n",
        "Зведений маніфест записується у out/tables/_manifest_tasks_17_19.csv і може бути прикріплений до звіту/README."
      ],
      "metadata": {
        "id": "RbPO8Uv9_G1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔍 Саніті-чек / маніфест артефактів Task17–19\n",
        "import os, time, glob\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "OUT = Path(\"out/tables\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 👇 Список очікуваних файлів по тасках (оновлюємо за потреби)\n",
        "EXPECTED = {\n",
        "    \"Task17\": [\n",
        "        \"EDA_types.csv\",\n",
        "        \"EDA_nulls.csv\",\n",
        "        \"EDA_pos_count.csv\",\n",
        "        \"EDA_role_group_count.csv\",\n",
        "        \"EDA_categorical_cardinality.csv\",\n",
        "        \"EDA_contamination.csv\",\n",
        "        \"EDA_quantiles_core.csv\",\n",
        "        \"EDA_range_violations.csv\",\n",
        "        \"EDA_core_strong_corr.csv\",\n",
        "        \"EDA_strong_corr_all_pearson.csv\",\n",
        "        \"EDA_strong_corr_all_spearman.csv\",\n",
        "        \"Task17_role_extremes_height_weight.csv\",\n",
        "        \"Task17_missing_bodymetrics.csv\",\n",
        "        \"Task17_value_outliers_byrole.csv\",\n",
        "        \"Task17_wage_outliers_byrole.csv\",\n",
        "        \"Task17_high_overall_but_weak_core_attrs.csv\",\n",
        "    ],\n",
        "    \"Task18\": [\n",
        "        \"Task18_features_base.csv\",\n",
        "        \"Task18_features_train.csv\",\n",
        "        \"Task18_featset_A_list.csv\",\n",
        "        \"Task18_featset_B_list.csv\",\n",
        "    ],\n",
        "    \"Task19\": [\n",
        "        \"Task19_feature_ranks_A.csv\",\n",
        "        \"Task19_feature_ranks_B.csv\",\n",
        "        \"Task19_selected_features_A.csv\",\n",
        "        \"Task19_selected_features_B.csv\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "STRICT = False  # True → падати з assert, якщо критичний файл відсутній/порожній\n",
        "\n",
        "rows = []\n",
        "for task, files in EXPECTED.items():\n",
        "    for name in files:\n",
        "        p = OUT / name\n",
        "        exists = p.exists()\n",
        "        size   = p.stat().st_size if exists else 0\n",
        "        mtime  = time.ctime(p.stat().st_mtime) if exists else \"-\"\n",
        "        head   = \"-\"\n",
        "        if exists and p.suffix == \".csv\" and size > 0:\n",
        "            try:\n",
        "                head = pd.read_csv(p, nrows=3).shape[0]\n",
        "            except Exception:\n",
        "                head = \"?\"\n",
        "        rows.append({\n",
        "            \"task\": task,\n",
        "            \"path\": str(p),\n",
        "            \"exists\": exists,\n",
        "            \"size\": int(size),\n",
        "            \"head_rows\": head,\n",
        "            \"mtime\": mtime\n",
        "        })\n",
        "\n",
        "# Додаткові CSV у каталозі, яких немає в EXPECTED (щоб не загубити щось корисне)\n",
        "all_csv = set(glob.glob(str(OUT / \"*.csv\")))\n",
        "expected_csv = set(str(OUT / f) for fs in EXPECTED.values() for f in fs)\n",
        "extra = sorted(all_csv - expected_csv)\n",
        "\n",
        "man = pd.DataFrame(rows).sort_values([\"task\", \"path\"])\n",
        "man.to_csv(OUT / \"_manifest_tasks_17_19.csv\", index=False)\n",
        "\n",
        "# Зручний друк\n",
        "ok = True\n",
        "for _, r in man.iterrows():\n",
        "    mark = \"✅\" if (r[\"exists\"] and r[\"size\"] > 0) else \"❌\"\n",
        "    print(f'{mark} {r[\"task\"]:<6} | {os.path.basename(r[\"path\"]):45s} | size={r[\"size\"]:>7} | head={r[\"head_rows\"]!s:<3} | {r[\"mtime\"]}')\n",
        "    if STRICT and not (r[\"exists\"] and r[\"size\"] > 0):\n",
        "        ok = False\n",
        "\n",
        "if extra:\n",
        "    print(\"\\nℹ️  Додаткові CSV (не у списку EXPECTED):\")\n",
        "    for p in extra:\n",
        "        print(\"   •\", os.path.basename(p))\n",
        "\n",
        "if STRICT:\n",
        "    assert ok, \"Є відсутні або порожні файли у критичних артефактах.\"\n",
        "else:\n",
        "    print(\"\\nМаніфест збережено → out/tables/_manifest_tasks_17_19.csv\")\n"
      ],
      "metadata": {
        "id": "YijOSnV0LpDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мета блоку: «Синхронізація результатів у GitHub і підготовка до моделювання»\n",
        "\n",
        "Навіщо:\n",
        "Зафіксувати всі артефакти з Task17–19 у репозиторії як «джерело правди», щоб у наступному ноутбуці ми не готували дані повторно, а просто зчитали готові таблиці з GitHub. Це підвищує відтворюваність і спрощує перевірку.\n",
        "\n",
        "Що саме відправляємо (очікувано):\n",
        "\n",
        "out/tables/Task18_features_base.csv\n",
        "\n",
        "out/tables/Task18_features_train.csv\n",
        "\n",
        "out/tables/Task18_featset_A_list.csv, Task18_featset_B_list.csv\n",
        "\n",
        "out/tables/Task19_feature_ranks_A.csv, Task19_feature_ranks_B.csv\n",
        "\n",
        "out/tables/Task19_selected_features_A.csv, Task19_selected_features_B.csv\n",
        "\n",
        "допоміжні EDA-звіти (EDA_*.csv) та маніфест _manifest_tasks_17_19.csv\n",
        "\n",
        "Якщо деякі CSV відсутні — це сигнал, що відповідні попередні клітинки не запускались або повернули пустий результат."
      ],
      "metadata": {
        "id": "KSk9bjThHw7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Sync Task17–19 артефактів у GitHub + підготовка до моделювання\n",
        "import os, glob, json, getpass, subprocess, shlex, time\n",
        "from pathlib import Path\n",
        "\n",
        "#  Налаштування\n",
        "WORKDIR = \"/content/project_fifa_players\"   # корінь клонованого репо\n",
        "BRANCH  = \"main\"                            # цільова гілка\n",
        "GLOBS   = [\"out/tables/*.csv\", \"out/figures/*.png\"]  # що пушимо (CSV/PNG)\n",
        "FORCE_ADD_IF_IGNORED = False                # True → git add -f\n",
        "\n",
        "# Вхідні для наступного ноутбука (класифікація)\n",
        "INPUTS = {\n",
        "    \"train_matrix\": \"out/tables/Task18_features_train.csv\",\n",
        "    \"featset_A\":    \"out/tables/Task19_selected_features_A.csv\",\n",
        "    \"featset_B\":    \"out/tables/Task19_selected_features_B.csv\",\n",
        "}\n",
        "\n",
        "#  Утиліти\n",
        "def run(cmd, check=True, capture=False):\n",
        "    print(\"$\", cmd)\n",
        "    if capture:\n",
        "        return subprocess.run(shlex.split(cmd), check=check, capture_output=True, text=True)\n",
        "    return subprocess.run(shlex.split(cmd), check=check)\n",
        "\n",
        "#  0) Робоча тека\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "os.chdir(WORKDIR)\n",
        "print(\"pwd:\", os.getcwd())\n",
        "\n",
        "#  1) Переконаємось, що потрібні файли існують\n",
        "missing = [p for p in INPUTS.values() if not Path(p).exists()]\n",
        "if missing:\n",
        "    print(\"⚠️  Відсутні необхідні файли для моделювання:\", missing)\n",
        "else:\n",
        "    # створимо службовий JSON з вхідними шляхами\n",
        "    Path(\"modeling\").mkdir(exist_ok=True)\n",
        "    with open(\"modeling/inputs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(INPUTS, f, ensure_ascii=False, indent=2)\n",
        "    print(\"✅ Створено modeling/inputs.json\")\n",
        "\n",
        "#  2) Конфіг локального автора комітів\n",
        "run('git config user.name \"rvkushnir\"', check=False)\n",
        "run('git config user.email \"rvkushnir@gmail.com\"', check=False)\n",
        "\n",
        "#  3) Додамо артефакти у staging\n",
        "added_files = []\n",
        "for pat in GLOBS + [\"modeling/inputs.json\"]:\n",
        "    # якщо шаблон → розгортаємо\n",
        "    paths = glob.glob(pat) if any(ch in pat for ch in \"*?[]\") else [pat]\n",
        "    paths = [p for p in paths if Path(p).exists()]\n",
        "    if not paths:\n",
        "        continue\n",
        "    if FORCE_ADD_IF_IGNORED:\n",
        "        run(f'git add -f {\" \".join(shlex.quote(p) for p in paths)}', check=False)\n",
        "    else:\n",
        "        run(f'git add {\" \".join(shlex.quote(p) for p in paths)}', check=False)\n",
        "    added_files.extend(paths)\n",
        "\n",
        "if not added_files:\n",
        "    print(\"ℹ️  Немає що додавати (файлів за шаблонами не знайдено).\")\n",
        "\n",
        "#  4) Коміт (лише якщо є staged-зміни)\n",
        "has_staged = subprocess.run(shlex.split(\"git diff --cached --quiet\")).returncode != 0\n",
        "if has_staged:\n",
        "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    run(f'git commit -m \"Sync Task17–19 artifacts + modeling/inputs.json ({ts})\"', check=False)\n",
        "else:\n",
        "    print(\"ℹ️  Немає нових staged-змін — коміт пропущено.\")\n",
        "\n",
        "#  5) Актуалізація та rebase поверх origin/main\n",
        "run(f\"git fetch origin {BRANCH}\", check=False)\n",
        "run(f\"git rebase origin/{BRANCH}\", check=False)\n",
        "\n",
        "#  6) Пуш з токеном\n",
        "if \"GH_TOKEN\" not in os.environ or not os.environ[\"GH_TOKEN\"]:\n",
        "    os.environ[\"GH_TOKEN\"] = getpass.getpass(\"GitHub token (приховано): \")\n",
        "\n",
        "push_url = f'https://x-access-token:{os.environ[\"GH_TOKEN\"]}@github.com/rvkushnir/project_fifa_players.git'\n",
        "run(f'git push \"{push_url}\" HEAD:{BRANCH}', check=False)\n",
        "\n",
        "#  7) Звіт: що є у HEAD\n",
        "res = run('git ls-tree -r HEAD --name-only', check=False, capture=True)\n",
        "head_files = [p for p in res.stdout.splitlines() if p.startswith((\"out/tables/\",\"out/figures/\",\"modeling/\"))]\n",
        "print(\"\\n✅ У HEAD тепер є:\")\n",
        "for p in head_files:\n",
        "    print(\" •\", p)\n"
      ],
      "metadata": {
        "id": "VIhMp35p_3oI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}